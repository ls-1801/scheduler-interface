package k8s.sparkoperator;/*
 * Kubernetes
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: v1.21.1
 *
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonValue;
import io.swagger.annotations.ApiModelProperty;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Objects;

/**
 * V1beta2SparkApplicationSpec
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2022-01-30T13:32" +
        ":37.998Z[Etc/UTC]")
public class V1beta2SparkApplicationSpec {

    private List<String> arguments = null;


    private String batchScheduler;


    private V1beta2SparkApplicationSpecBatchSchedulerOptions batchSchedulerOptions;


    private V1beta2SparkApplicationSpecDeps deps;


    private V1beta2SparkApplicationSpecDriver driver;


    private V1beta2SparkApplicationSpecDynamicAllocation dynamicAllocation;


    private V1beta2SparkApplicationSpecExecutor executor;


    private Integer failureRetries;


    private Map<String, String> hadoopConf = null;


    private String hadoopConfigMap;


    private String image;


    private String imagePullPolicy;


    private List<String> imagePullSecrets = null;


    private String mainApplicationFile;


    private String mainClass;


    private String memoryOverheadFactor;
    private ModeEnum mode;
    private V1beta2SparkApplicationSpecMonitoring monitoring;
    private Map<String, String> nodeSelector = null;
    private String proxyUser;
    private PythonVersionEnum pythonVersion;
    private V1beta2SparkApplicationSpecRestartPolicy restartPolicy;
    private Long retryInterval;
    private Map<String, String> sparkConf = null;
    private String sparkConfigMap;
    private V1beta2SparkApplicationSpecSparkUIOptions sparkUIOptions;
    private String sparkVersion;
    private Long timeToLiveSeconds;
    private TypeEnum type;
    private List<V1beta2SparkApplicationSpecVolumes> volumes = null;

    public V1beta2SparkApplicationSpec arguments(List<String> arguments) {

        this.arguments = arguments;
        return this;
    }

    public V1beta2SparkApplicationSpec addArgumentsItem(String argumentsItem) {
        if (this.arguments == null) {
            this.arguments = new ArrayList<>();
        }
        this.arguments.add(argumentsItem);
        return this;
    }

    /**
     * Get arguments
     *
     * @return arguments
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public List<String> getArguments() {
        return arguments;
    }

    public void setArguments(List<String> arguments) {
        this.arguments = arguments;
    }

    public V1beta2SparkApplicationSpec batchScheduler(String batchScheduler) {

        this.batchScheduler = batchScheduler;
        return this;
    }

    /**
     * Get batchScheduler
     *
     * @return batchScheduler
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getBatchScheduler() {
        return batchScheduler;
    }

    public void setBatchScheduler(String batchScheduler) {
        this.batchScheduler = batchScheduler;
    }

    public V1beta2SparkApplicationSpec batchSchedulerOptions(V1beta2SparkApplicationSpecBatchSchedulerOptions batchSchedulerOptions) {

        this.batchSchedulerOptions = batchSchedulerOptions;
        return this;
    }

    /**
     * Get batchSchedulerOptions
     *
     * @return batchSchedulerOptions
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public V1beta2SparkApplicationSpecBatchSchedulerOptions getBatchSchedulerOptions() {
        return batchSchedulerOptions;
    }

    public void setBatchSchedulerOptions(V1beta2SparkApplicationSpecBatchSchedulerOptions batchSchedulerOptions) {
        this.batchSchedulerOptions = batchSchedulerOptions;
    }

    public V1beta2SparkApplicationSpec deps(V1beta2SparkApplicationSpecDeps deps) {

        this.deps = deps;
        return this;
    }

    /**
     * Get deps
     *
     * @return deps
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public V1beta2SparkApplicationSpecDeps getDeps() {
        return deps;
    }

    public void setDeps(V1beta2SparkApplicationSpecDeps deps) {
        this.deps = deps;
    }

    public V1beta2SparkApplicationSpec driver(V1beta2SparkApplicationSpecDriver driver) {

        this.driver = driver;
        return this;
    }

    /**
     * Get driver
     *
     * @return driver
     **/
    @ApiModelProperty(required = true, value = "")

    public V1beta2SparkApplicationSpecDriver getDriver() {
        return driver;
    }

    public void setDriver(V1beta2SparkApplicationSpecDriver driver) {
        this.driver = driver;
    }

    public V1beta2SparkApplicationSpec dynamicAllocation(V1beta2SparkApplicationSpecDynamicAllocation dynamicAllocation) {

        this.dynamicAllocation = dynamicAllocation;
        return this;
    }

    /**
     * Get dynamicAllocation
     *
     * @return dynamicAllocation
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public V1beta2SparkApplicationSpecDynamicAllocation getDynamicAllocation() {
        return dynamicAllocation;
    }

    public void setDynamicAllocation(V1beta2SparkApplicationSpecDynamicAllocation dynamicAllocation) {
        this.dynamicAllocation = dynamicAllocation;
    }

    public V1beta2SparkApplicationSpec executor(V1beta2SparkApplicationSpecExecutor executor) {

        this.executor = executor;
        return this;
    }

    /**
     * Get executor
     *
     * @return executor
     **/
    @ApiModelProperty(required = true, value = "")

    public V1beta2SparkApplicationSpecExecutor getExecutor() {
        return executor;
    }

    public void setExecutor(V1beta2SparkApplicationSpecExecutor executor) {
        this.executor = executor;
    }

    public V1beta2SparkApplicationSpec failureRetries(Integer failureRetries) {

        this.failureRetries = failureRetries;
        return this;
    }

    /**
     * Get failureRetries
     *
     * @return failureRetries
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public Integer getFailureRetries() {
        return failureRetries;
    }

    public void setFailureRetries(Integer failureRetries) {
        this.failureRetries = failureRetries;
    }

    public V1beta2SparkApplicationSpec hadoopConf(Map<String, String> hadoopConf) {

        this.hadoopConf = hadoopConf;
        return this;
    }

    public V1beta2SparkApplicationSpec putHadoopConfItem(String key, String hadoopConfItem) {
        if (this.hadoopConf == null) {
            this.hadoopConf = new HashMap<>();
        }
        this.hadoopConf.put(key, hadoopConfItem);
        return this;
    }

    /**
     * Get hadoopConf
     *
     * @return hadoopConf
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public Map<String, String> getHadoopConf() {
        return hadoopConf;
    }

    public void setHadoopConf(Map<String, String> hadoopConf) {
        this.hadoopConf = hadoopConf;
    }

    public V1beta2SparkApplicationSpec hadoopConfigMap(String hadoopConfigMap) {

        this.hadoopConfigMap = hadoopConfigMap;
        return this;
    }

    /**
     * Get hadoopConfigMap
     *
     * @return hadoopConfigMap
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getHadoopConfigMap() {
        return hadoopConfigMap;
    }

    public void setHadoopConfigMap(String hadoopConfigMap) {
        this.hadoopConfigMap = hadoopConfigMap;
    }

    public V1beta2SparkApplicationSpec image(String image) {

        this.image = image;
        return this;
    }

    /**
     * Get image
     *
     * @return image
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getImage() {
        return image;
    }

    public void setImage(String image) {
        this.image = image;
    }

    public V1beta2SparkApplicationSpec imagePullPolicy(String imagePullPolicy) {

        this.imagePullPolicy = imagePullPolicy;
        return this;
    }

    /**
     * Get imagePullPolicy
     *
     * @return imagePullPolicy
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getImagePullPolicy() {
        return imagePullPolicy;
    }

    public void setImagePullPolicy(String imagePullPolicy) {
        this.imagePullPolicy = imagePullPolicy;
    }

    public V1beta2SparkApplicationSpec imagePullSecrets(List<String> imagePullSecrets) {

        this.imagePullSecrets = imagePullSecrets;
        return this;
    }

    public V1beta2SparkApplicationSpec addImagePullSecretsItem(String imagePullSecretsItem) {
        if (this.imagePullSecrets == null) {
            this.imagePullSecrets = new ArrayList<>();
        }
        this.imagePullSecrets.add(imagePullSecretsItem);
        return this;
    }

    /**
     * Get imagePullSecrets
     *
     * @return imagePullSecrets
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public List<String> getImagePullSecrets() {
        return imagePullSecrets;
    }

    public void setImagePullSecrets(List<String> imagePullSecrets) {
        this.imagePullSecrets = imagePullSecrets;
    }

    public V1beta2SparkApplicationSpec mainApplicationFile(String mainApplicationFile) {

        this.mainApplicationFile = mainApplicationFile;
        return this;
    }

    /**
     * Get mainApplicationFile
     *
     * @return mainApplicationFile
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getMainApplicationFile() {
        return mainApplicationFile;
    }

    public void setMainApplicationFile(String mainApplicationFile) {
        this.mainApplicationFile = mainApplicationFile;
    }

    public V1beta2SparkApplicationSpec mainClass(String mainClass) {

        this.mainClass = mainClass;
        return this;
    }

    /**
     * Get mainClass
     *
     * @return mainClass
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getMainClass() {
        return mainClass;
    }

    public void setMainClass(String mainClass) {
        this.mainClass = mainClass;
    }

    public V1beta2SparkApplicationSpec memoryOverheadFactor(String memoryOverheadFactor) {

        this.memoryOverheadFactor = memoryOverheadFactor;
        return this;
    }

    /**
     * Get memoryOverheadFactor
     *
     * @return memoryOverheadFactor
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getMemoryOverheadFactor() {
        return memoryOverheadFactor;
    }

    public void setMemoryOverheadFactor(String memoryOverheadFactor) {
        this.memoryOverheadFactor = memoryOverheadFactor;
    }

    public V1beta2SparkApplicationSpec mode(ModeEnum mode) {

        this.mode = mode;
        return this;
    }

    /**
     * Get mode
     *
     * @return mode
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public ModeEnum getMode() {
        return mode;
    }

    public void setMode(ModeEnum mode) {
        this.mode = mode;
    }

    public V1beta2SparkApplicationSpec monitoring(V1beta2SparkApplicationSpecMonitoring monitoring) {

        this.monitoring = monitoring;
        return this;
    }

    /**
     * Get monitoring
     *
     * @return monitoring
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public V1beta2SparkApplicationSpecMonitoring getMonitoring() {
        return monitoring;
    }

    public void setMonitoring(V1beta2SparkApplicationSpecMonitoring monitoring) {
        this.monitoring = monitoring;
    }

    public V1beta2SparkApplicationSpec nodeSelector(Map<String, String> nodeSelector) {

        this.nodeSelector = nodeSelector;
        return this;
    }

    public V1beta2SparkApplicationSpec putNodeSelectorItem(String key, String nodeSelectorItem) {
        if (this.nodeSelector == null) {
            this.nodeSelector = new HashMap<>();
        }
        this.nodeSelector.put(key, nodeSelectorItem);
        return this;
    }

    /**
     * Get nodeSelector
     *
     * @return nodeSelector
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public Map<String, String> getNodeSelector() {
        return nodeSelector;
    }

    public void setNodeSelector(Map<String, String> nodeSelector) {
        this.nodeSelector = nodeSelector;
    }

    public V1beta2SparkApplicationSpec proxyUser(String proxyUser) {

        this.proxyUser = proxyUser;
        return this;
    }

    /**
     * Get proxyUser
     *
     * @return proxyUser
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getProxyUser() {
        return proxyUser;
    }

    public void setProxyUser(String proxyUser) {
        this.proxyUser = proxyUser;
    }

    public V1beta2SparkApplicationSpec pythonVersion(PythonVersionEnum pythonVersion) {

        this.pythonVersion = pythonVersion;
        return this;
    }

    /**
     * Get pythonVersion
     *
     * @return pythonVersion
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public PythonVersionEnum getPythonVersion() {
        return pythonVersion;
    }

    public void setPythonVersion(PythonVersionEnum pythonVersion) {
        this.pythonVersion = pythonVersion;
    }

    public V1beta2SparkApplicationSpec restartPolicy(V1beta2SparkApplicationSpecRestartPolicy restartPolicy) {

        this.restartPolicy = restartPolicy;
        return this;
    }

    /**
     * Get restartPolicy
     *
     * @return restartPolicy
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public V1beta2SparkApplicationSpecRestartPolicy getRestartPolicy() {
        return restartPolicy;
    }

    public void setRestartPolicy(V1beta2SparkApplicationSpecRestartPolicy restartPolicy) {
        this.restartPolicy = restartPolicy;
    }

    public V1beta2SparkApplicationSpec retryInterval(Long retryInterval) {

        this.retryInterval = retryInterval;
        return this;
    }

    /**
     * Get retryInterval
     *
     * @return retryInterval
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public Long getRetryInterval() {
        return retryInterval;
    }

    public void setRetryInterval(Long retryInterval) {
        this.retryInterval = retryInterval;
    }

    public V1beta2SparkApplicationSpec sparkConf(Map<String, String> sparkConf) {

        this.sparkConf = sparkConf;
        return this;
    }

    public V1beta2SparkApplicationSpec putSparkConfItem(String key, String sparkConfItem) {
        if (this.sparkConf == null) {
            this.sparkConf = new HashMap<>();
        }
        this.sparkConf.put(key, sparkConfItem);
        return this;
    }

    /**
     * Get sparkConf
     *
     * @return sparkConf
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public Map<String, String> getSparkConf() {
        return sparkConf;
    }

    public void setSparkConf(Map<String, String> sparkConf) {
        this.sparkConf = sparkConf;
    }

    public V1beta2SparkApplicationSpec sparkConfigMap(String sparkConfigMap) {

        this.sparkConfigMap = sparkConfigMap;
        return this;
    }

    /**
     * Get sparkConfigMap
     *
     * @return sparkConfigMap
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public String getSparkConfigMap() {
        return sparkConfigMap;
    }

    public void setSparkConfigMap(String sparkConfigMap) {
        this.sparkConfigMap = sparkConfigMap;
    }

    public V1beta2SparkApplicationSpec sparkUIOptions(V1beta2SparkApplicationSpecSparkUIOptions sparkUIOptions) {

        this.sparkUIOptions = sparkUIOptions;
        return this;
    }

    /**
     * Get sparkUIOptions
     *
     * @return sparkUIOptions
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public V1beta2SparkApplicationSpecSparkUIOptions getSparkUIOptions() {
        return sparkUIOptions;
    }

    public void setSparkUIOptions(V1beta2SparkApplicationSpecSparkUIOptions sparkUIOptions) {
        this.sparkUIOptions = sparkUIOptions;
    }

    public V1beta2SparkApplicationSpec sparkVersion(String sparkVersion) {

        this.sparkVersion = sparkVersion;
        return this;
    }

    /**
     * Get sparkVersion
     *
     * @return sparkVersion
     **/
    @ApiModelProperty(required = true, value = "")

    public String getSparkVersion() {
        return sparkVersion;
    }

    public void setSparkVersion(String sparkVersion) {
        this.sparkVersion = sparkVersion;
    }

    public V1beta2SparkApplicationSpec timeToLiveSeconds(Long timeToLiveSeconds) {

        this.timeToLiveSeconds = timeToLiveSeconds;
        return this;
    }

    /**
     * Get timeToLiveSeconds
     *
     * @return timeToLiveSeconds
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public Long getTimeToLiveSeconds() {
        return timeToLiveSeconds;
    }

    public void setTimeToLiveSeconds(Long timeToLiveSeconds) {
        this.timeToLiveSeconds = timeToLiveSeconds;
    }

    public V1beta2SparkApplicationSpec type(TypeEnum type) {

        this.type = type;
        return this;
    }

    /**
     * Get type
     *
     * @return type
     **/
    @ApiModelProperty(required = true, value = "")

    public TypeEnum getType() {
        return type;
    }

    public void setType(TypeEnum type) {
        this.type = type;
    }

    public V1beta2SparkApplicationSpec volumes(List<V1beta2SparkApplicationSpecVolumes> volumes) {

        this.volumes = volumes;
        return this;
    }

    public V1beta2SparkApplicationSpec addVolumesItem(V1beta2SparkApplicationSpecVolumes volumesItem) {
        if (this.volumes == null) {
            this.volumes = new ArrayList<>();
        }
        this.volumes.add(volumesItem);
        return this;
    }

    /**
     * Get volumes
     *
     * @return volumes
     **/
    @javax.annotation.Nullable
    @ApiModelProperty(value = "")

    public List<V1beta2SparkApplicationSpecVolumes> getVolumes() {
        return volumes;
    }

    public void setVolumes(List<V1beta2SparkApplicationSpecVolumes> volumes) {
        this.volumes = volumes;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) {
            return true;
        }
        if (o == null || getClass() != o.getClass()) {
            return false;
        }
        V1beta2SparkApplicationSpec v1beta2SparkApplicationSpec = (V1beta2SparkApplicationSpec) o;
        return Objects.equals(this.arguments, v1beta2SparkApplicationSpec.arguments) &&
                Objects.equals(this.batchScheduler, v1beta2SparkApplicationSpec.batchScheduler) &&
                Objects.equals(this.batchSchedulerOptions, v1beta2SparkApplicationSpec.batchSchedulerOptions) &&
                Objects.equals(this.deps, v1beta2SparkApplicationSpec.deps) &&
                Objects.equals(this.driver, v1beta2SparkApplicationSpec.driver) &&
                Objects.equals(this.dynamicAllocation, v1beta2SparkApplicationSpec.dynamicAllocation) &&
                Objects.equals(this.executor, v1beta2SparkApplicationSpec.executor) &&
                Objects.equals(this.failureRetries, v1beta2SparkApplicationSpec.failureRetries) &&
                Objects.equals(this.hadoopConf, v1beta2SparkApplicationSpec.hadoopConf) &&
                Objects.equals(this.hadoopConfigMap, v1beta2SparkApplicationSpec.hadoopConfigMap) &&
                Objects.equals(this.image, v1beta2SparkApplicationSpec.image) &&
                Objects.equals(this.imagePullPolicy, v1beta2SparkApplicationSpec.imagePullPolicy) &&
                Objects.equals(this.imagePullSecrets, v1beta2SparkApplicationSpec.imagePullSecrets) &&
                Objects.equals(this.mainApplicationFile, v1beta2SparkApplicationSpec.mainApplicationFile) &&
                Objects.equals(this.mainClass, v1beta2SparkApplicationSpec.mainClass) &&
                Objects.equals(this.memoryOverheadFactor, v1beta2SparkApplicationSpec.memoryOverheadFactor) &&
                Objects.equals(this.mode, v1beta2SparkApplicationSpec.mode) &&
                Objects.equals(this.monitoring, v1beta2SparkApplicationSpec.monitoring) &&
                Objects.equals(this.nodeSelector, v1beta2SparkApplicationSpec.nodeSelector) &&
                Objects.equals(this.proxyUser, v1beta2SparkApplicationSpec.proxyUser) &&
                Objects.equals(this.pythonVersion, v1beta2SparkApplicationSpec.pythonVersion) &&
                Objects.equals(this.restartPolicy, v1beta2SparkApplicationSpec.restartPolicy) &&
                Objects.equals(this.retryInterval, v1beta2SparkApplicationSpec.retryInterval) &&
                Objects.equals(this.sparkConf, v1beta2SparkApplicationSpec.sparkConf) &&
                Objects.equals(this.sparkConfigMap, v1beta2SparkApplicationSpec.sparkConfigMap) &&
                Objects.equals(this.sparkUIOptions, v1beta2SparkApplicationSpec.sparkUIOptions) &&
                Objects.equals(this.sparkVersion, v1beta2SparkApplicationSpec.sparkVersion) &&
                Objects.equals(this.timeToLiveSeconds, v1beta2SparkApplicationSpec.timeToLiveSeconds) &&
                Objects.equals(this.type, v1beta2SparkApplicationSpec.type) &&
                Objects.equals(this.volumes, v1beta2SparkApplicationSpec.volumes);
    }

    @Override
    public int hashCode() {
        return Objects.hash(arguments, batchScheduler, batchSchedulerOptions, deps, driver, dynamicAllocation,
                executor, failureRetries, hadoopConf, hadoopConfigMap, image, imagePullPolicy, imagePullSecrets,
                mainApplicationFile, mainClass, memoryOverheadFactor, mode, monitoring, nodeSelector, proxyUser,
                pythonVersion, restartPolicy, retryInterval, sparkConf, sparkConfigMap, sparkUIOptions, sparkVersion,
                timeToLiveSeconds, type, volumes);
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("class V1beta2SparkApplicationSpec {\n");
        sb.append("    arguments: ").append(toIndentedString(arguments)).append("\n");
        sb.append("    batchScheduler: ").append(toIndentedString(batchScheduler)).append("\n");
        sb.append("    batchSchedulerOptions: ").append(toIndentedString(batchSchedulerOptions)).append("\n");
        sb.append("    deps: ").append(toIndentedString(deps)).append("\n");
        sb.append("    driver: ").append(toIndentedString(driver)).append("\n");
        sb.append("    dynamicAllocation: ").append(toIndentedString(dynamicAllocation)).append("\n");
        sb.append("    executor: ").append(toIndentedString(executor)).append("\n");
        sb.append("    failureRetries: ").append(toIndentedString(failureRetries)).append("\n");
        sb.append("    hadoopConf: ").append(toIndentedString(hadoopConf)).append("\n");
        sb.append("    hadoopConfigMap: ").append(toIndentedString(hadoopConfigMap)).append("\n");
        sb.append("    image: ").append(toIndentedString(image)).append("\n");
        sb.append("    imagePullPolicy: ").append(toIndentedString(imagePullPolicy)).append("\n");
        sb.append("    imagePullSecrets: ").append(toIndentedString(imagePullSecrets)).append("\n");
        sb.append("    mainApplicationFile: ").append(toIndentedString(mainApplicationFile)).append("\n");
        sb.append("    mainClass: ").append(toIndentedString(mainClass)).append("\n");
        sb.append("    memoryOverheadFactor: ").append(toIndentedString(memoryOverheadFactor)).append("\n");
        sb.append("    mode: ").append(toIndentedString(mode)).append("\n");
        sb.append("    monitoring: ").append(toIndentedString(monitoring)).append("\n");
        sb.append("    nodeSelector: ").append(toIndentedString(nodeSelector)).append("\n");
        sb.append("    proxyUser: ").append(toIndentedString(proxyUser)).append("\n");
        sb.append("    pythonVersion: ").append(toIndentedString(pythonVersion)).append("\n");
        sb.append("    restartPolicy: ").append(toIndentedString(restartPolicy)).append("\n");
        sb.append("    retryInterval: ").append(toIndentedString(retryInterval)).append("\n");
        sb.append("    sparkConf: ").append(toIndentedString(sparkConf)).append("\n");
        sb.append("    sparkConfigMap: ").append(toIndentedString(sparkConfigMap)).append("\n");
        sb.append("    sparkUIOptions: ").append(toIndentedString(sparkUIOptions)).append("\n");
        sb.append("    sparkVersion: ").append(toIndentedString(sparkVersion)).append("\n");
        sb.append("    timeToLiveSeconds: ").append(toIndentedString(timeToLiveSeconds)).append("\n");
        sb.append("    type: ").append(toIndentedString(type)).append("\n");
        sb.append("    volumes: ").append(toIndentedString(volumes)).append("\n");
        sb.append("}");
        return sb.toString();
    }

    /**
     * Convert the given object to string with each line indented by 4 spaces
     * (except the first line).
     */
    private String toIndentedString(Object o) {
        if (o == null) {
            return "null";
        }
        return o.toString().replace("\n", "\n    ");
    }

    /**
     * Gets or Sets mode
     */
    public enum ModeEnum {
        @JsonProperty("cluster")
        CLUSTER("cluster"),

        @JsonProperty("client")
        CLIENT("client");

        private String value;

        ModeEnum(String value) {
            this.value = value;
        }

        public static ModeEnum fromValue(String value) {
            for (ModeEnum b : ModeEnum.values()) {
                if (b.value.equals(value)) {
                    return b;
                }
            }
            throw new IllegalArgumentException("Unexpected value '" + value + "'");
        }

        @JsonValue
        public String getValue() {
            return value;
        }

        @Override
        public String toString() {
            return String.valueOf(value);
        }


    }


    /**
     * Gets or Sets pythonVersion
     */
    public enum PythonVersionEnum {
        @JsonProperty("2")
        _2("2"),

        @JsonProperty("3")
        _3("3");

        private String value;

        PythonVersionEnum(String value) {
            this.value = value;
        }

        public static PythonVersionEnum fromValue(String value) {
            for (PythonVersionEnum b : PythonVersionEnum.values()) {
                if (b.value.equals(value)) {
                    return b;
                }
            }
            throw new IllegalArgumentException("Unexpected value '" + value + "'");
        }

        @JsonValue
        public String getValue() {
            return value;
        }

        @Override
        public String toString() {
            return String.valueOf(value);
        }

    }

    /**
     * Gets or Sets type
     */
    public enum TypeEnum {
        @JsonProperty("Java")
        JAVA("Java"),

        @JsonProperty("Python")
        PYTHON("Python"),

        @JsonProperty("Scala")
        SCALA("Scala"),

        @JsonProperty("R")
        R("R");

        private String value;

        TypeEnum(String value) {
            this.value = value;
        }

        public static TypeEnum fromValue(String value) {
            for (TypeEnum b : TypeEnum.values()) {
                if (b.value.equals(value)) {
                    return b;
                }
            }
            throw new IllegalArgumentException("Unexpected value '" + value + "'");
        }

        @JsonValue
        public String getValue() {
            return value;
        }

        @Override
        public String toString() {
            return String.valueOf(value);
        }

    }

}

